Title:	The Psychological Report Proper
Author:	W. Howard Buddin Jr., Ph.D.
Category:           	Article
Date:                   	11/08/2013 
Tags:                   	practice, report writing 


I don't write great reports. Maybe I don't even write reports that are *good* by some standards. I'm okay with that. I have the rest of my professional life to evolve, and hopefully most of those years will entail continual embetterment of a cromulent skill set.[^1]   

Regardless of where I stand on the continuum of report-writing greatness, judging the fitness of reports, even those written by competent neuropsychologists, can be tough, and is fraught with subjectivity. There's a question here, too: just exactly where do we get our ideas about what a neuropsychological report should be/contain/say? After all, there is no single, universally agreed upon standard for neuropsychological reports.[^2] So, are there *objective* criteria, then, by which we can measure reports? 

Turns out, there are. Kind of. Jump into the way-back machine to 1953 and give this read by Lodge (1953) a try. The article might seem little more than an anachronism that we can laugh at a bit, but it serves a purpose, as well. It provides us with some sense about where our ideas came from in terms of report structure, what information they should contain, how to go about the *process* of writing a report, and so on.

Now let's turn things around: maybe it's easier to define factors that make a report *not-great* instead of the other way around. These are *much* easier to spot because we've all written patently bad reports at some point; supervisors see them on a regular basis. Since the "bad" is easier to spot than the "good," maybe examining reports from that perspective would be a little easier and more informative. 

Let's make a list. 
[^disclaimer]

Please note, this list is subjective and certainly not exhaustive. Also, you might disagree or even take umbrage with some points or statements. Such is the nature of the beast. Finally, the following list is unordered, meaning that there is no hierarchy to these points. 

<h2 style="textalign:center; font-size:30px"> The List </h2>


### Page Length ###

#### The Problem ####


Let's start this off right: nothing is more off-putting than receiving a 30–page report. Some people believe that longer means better. Maybe sometimes longer *is* better. Sometimes reports really *do* need some extra length. 

I once read a report that was unnecessarily long due to the practitioner listing *each* of his rule-outs, followed by a discourse of why "A diagnoses of Disorder x was ruled out due to..." and so on. I generally just assume that you've done your due diligence, and that what everyone cares about is *treatment and resolution*.  

#### How to Combat the Problem ####

Write shorter reports. Better than that, write appropriately concise reports. {++Really, it's not lengthy reports, per se, that are a problem. It is the "lengthy report that could be shorter if it weren't for the person's complete life history" reports that are harder to stomach.++}{>>@whb - 2013.11.07<<}

Neuropsychological reports don't need to be (and probably shouldn't be) bulleted lists (e.g., inpatient consult notes), but they also don't need to showcase your expository writing skills. There is one literary analogy that might help put this possible solution into perspective: write like Hemingway, not like Tolstoy. That is, be concise; choose sentences that convey the necessary information with minimal or no superfluous embellishments.

***

{==Marc, I added a new item below, in the event that we abandon the last two. Track changes were off for this one, for obvious reasons==}{>>@whb - 2013.11.07<<}

### The Recipient-Agnostic Report ###

#### The Problem ####

It's a good idea to have a sense of who will ultimately read your report. Neglecting your audience and writing a summary section with the same language or tone is a problem. 

#### How to Combat the Problem ####

There is not a simple fix. Broadly, the idea is to know/consider your audience, in much the same way you would if you were preparing a presentation on e.g., sequelae of vascular dementia. That is, it would be important to know if the audience were college students versus members of the community versus your colleagues. Each audience *should* get a presentation tailored to both their knowledge of the subject and to what information would be most useful or helpful to them.  

{==### The Single Cell Error ###

#### The Problem ####
==}{>>this has nothing to do with report writing, this has to do with thinking and interpreting data....i think this topic deserves a separate post....we also border on "mightier than thou" especially if we don't provide references...at the heart of this is Paul Meehl...SMT - 2013-11-06 16:43:15<<}

The *single cell error* occurs when one or more conclusions are made based on the outcome of a single data point. It can be fueled by *confirmation bias*, which is when we are more or less compelled to acknowledge only the information that supports our working hypothesis, *and* ignore other information that might call into question the validity of that same hypothesis. It is expected that our patients will not usually produced nicely clustered scores that yield low intra-individual variability; scores are often fairly well dispersed. We could reasonably, in many cases, call the outlier scores (personal) strengths and/or weaknesses, depending on their magnitude and direction relative to that person's mean. Occasionally, though, clinicians are overcome by bias, and end up placing too much weight on one or two scores, resulting in a possible misdiagnosis.  

#### How to Combat the Problem ####

You probably learned about the single cell error, or some variety of it, at some point during your training. You also learned about confirmation bias. Don't just let your training slip away, or think that these things apply only to therapeutic settings. Apply a healthy dose of meta-awareness when you start to feel "certain" about a particular diagnosis. In fact, confirmation bias (and it's cousin, cognitive dissonance) predict that in some cases increased certainty and increased bias are correlated. If you find that you are sure about a diagnosis very early on, check with yourself first ("is there anything here that discredits my diagnosis?") and then run the case by a colleague or two. [^3] 

***

{==### Ignoring Data ###==}{>>this is also covered in Leezak  .  SMT - 2013-11-06 16:47:07<<}

#### The Problem ####


This one goes hand in hand with the last point, although it is of a slightly different flavor. *Ignoring data* is grievous, and occurs when relevant, potentially critical data are ignored for various reasons. One of those reasons *could* be due to the confirmation bias just mentioned, but it could also be the result of ignorance, ineptitude, or carelessness. The data in this scenario are brushed aside as unimportant to the clinical picture. Although the data might stand out clearly, they are ignored because, e.g., they do not appear to be germane to the patient's presenting complaints. 

#### How to Combat the Problem ####


A neuropsychological evaluation is about *discovery*. Occasionally you might find something that does not appear clinically relevant, at least with respect to the presenting complaint. Perhaps it *is* irrelevant, but you should only arrive at this conclusion after investigating the nature of the score(s) in question. Remember: incidental (as in "incidental finding") is *not* synonymous with *irrelevant*. 

## Summary ##

At the end of the day, it's probably easy to blow off this article. After all, no one reading it has *ever* written a report that was too long, had a biased edge to it, or flatly ignored data, right? Of course not. Maybe you know someone who has, though. Perhaps a poor, beleaguered trainee who just doesn't seem to "get it." Share it with them, and maybe keep it bookmarked for, you know, future beleaguered students. 

If there are "triggers" that help you recognize sub-par reports, and you would like to share them with others, please let us know. We can add it here or, if we receive enough of them, possibly write out a supplemental post or guide for the benefit of others. 

[^1]: If “embetterment” and “cromulent” aren’t familiar to you, it’s because you’ve not whiled away enough hours watching {++[++}{>>SMT - 2013-11-06 16:03:53<<}The Simpsons{++]++}{>>SMT - 2013-11-06 16:04:29<<}.

[^2]: Indeed, is there any aspect of our profession that enjoys such a status? None I can think of.

[^3]: Of course, if you consult with one or more colleagues, it is important to not present the case in a way that is “leading.” Introducing your case with a statement like “So here is a patently obvious case of Pick’s Disease” is leading. By contrast, asking “Hey, would you mind taking a look at this with me?” is not leading in the slightest. You won’t keep confirmation bias in check if you lead someone else off with your biased perspective.
{++

[The Simpsons]: http://simpsons.wikia.com/wiki/Made-up_words++}{>>SMT - 2013-11-06 16:04:04<<}
